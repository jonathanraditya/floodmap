{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules / Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import modules / library\n",
    "import os\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import string\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Function List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function List\n",
    "\n",
    "#For exploding file names into readable array (with index)\n",
    "def explodeArray(fileName, replaceParams, splitParams):\n",
    "    replaces=fileName.replace(replaceParams, '')\n",
    "    splitToArrays=replaces.split(splitParams)\n",
    "    return splitToArrays\n",
    "\n",
    "#Return md5 checksum from specific file path\n",
    "def md5(fname):\n",
    "    import hashlib\n",
    "    hash_md5=hashlib.md5()\n",
    "    with open(fname, 'rb') as f:\n",
    "        for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "            hash_md5.update(chunk)\n",
    "    return hash_md5.hexdigest()\n",
    "\n",
    "#Return first and second index of file in the exploded file array\n",
    "def searchFileIndex(coorName, explodedFileArray):\n",
    "    #search full name from array\n",
    "    #firstIndex=explodedFileArray.index(coorName)\n",
    "    firstIndex=0\n",
    "    secondIndex=0\n",
    "    for i in range(len(explodedFileArray)):\n",
    "        if explodedFileArray[i][3] == coorName:\n",
    "            if firstIndex==0:\n",
    "                firstIndex=i\n",
    "            else:\n",
    "                secondIndex=i\n",
    "    return firstIndex, secondIndex\n",
    "\n",
    "#Return file size from specific file path\n",
    "def fileSize(filePath):\n",
    "    import os\n",
    "    osStat=os.stat(filePath)\n",
    "    return osStat.st_size\n",
    "\n",
    "#Return last modified mtime from specific file path\n",
    "def lastModified(path):\n",
    "    import os\n",
    "    osStat=os.stat(filePath)\n",
    "    return osStat.st_mtime\n",
    "\n",
    "#Replace something (identifier) with blank\n",
    "def replaceWithBlank(contents, identifier):\n",
    "    return contents.replace(identifier, '')\n",
    "\n",
    "#Shortening long precision coordinates to precision used in crawled data\n",
    "def coordShortener(longCoord):\n",
    "    return round(longCoord, 4)\n",
    "\n",
    "#Generate array of date from initial date for x number of day(s)\n",
    "def hourDateGenerator(year, month, day, numbOfDays):\n",
    "    import datetime\n",
    "    from datetime import time, timedelta\n",
    "    initialDate=datetime.datetime(year, month, day)\n",
    "    generateAllHours=24*numbOfDays\n",
    "    resultArray=[]\n",
    "    for i in range(generateAllHours):\n",
    "        resultArray=resultArray+[initialDate+timedelta(hours=i)]\n",
    "    return resultArray\n",
    "\n",
    "#Return Date With Hours Offset from Initial Date\n",
    "def dateOffsetHours(year,month, day, hoursOffset):\n",
    "    import datetime\n",
    "    from datetime import time, timedelta\n",
    "    initialDate=datetime.datetime(year, month, day)\n",
    "    return initialDate+timedelta(hours=hoursOffset)\n",
    "\n",
    "#Generate alphanumerical random text for specific length\n",
    "def getRandomString(length):\n",
    "    import random\n",
    "    import string\n",
    "    letters=string.ascii_letters + string.digits\n",
    "    result_str=''.join(random.choice(letters) for i in range(length))\n",
    "    return result_str\n",
    "\n",
    "#Show folder entries in specific folder path\n",
    "def folderEntries(folderPath):\n",
    "    import os\n",
    "    result=[]\n",
    "    with os.scandir(folderPath) as entries:\n",
    "        for entry in entries:\n",
    "            result=result+[entry.name]\n",
    "    return result\n",
    "    \n",
    "#Check if there are duplicate entries in Array, and return clean array of No Duplicate data\n",
    "def checkAndReturnNoDuplicatesArray(someArray):\n",
    "    noFilesDuplicate=[]\n",
    "    collectDuplicate=[]\n",
    "    for i in range(len(someArray)):\n",
    "        if someArray[i] not in noFilesDuplicate:\n",
    "            noFilesDuplicate=noFilesDuplicate+[someArray[i]]\n",
    "        else:\n",
    "            collectDuplicate=collectDuplicate+[someArray[i]]\n",
    "    return noFilesDuplicate, collectDuplicate\n",
    "\n",
    "def openFile(filePath):\n",
    "    with open(filePath, 'r') as f:\n",
    "        result=f.read()\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Advanced Function\n",
    "\n",
    "#Exploit file name to extract latitude and longitude\n",
    "def exploitLatLong(fileName):\n",
    "    split1=explodeArray(fileName, ']', '[')\n",
    "    split2=splitToArray(split1, '', ',')\n",
    "    return split2[0], split2[1]\n",
    "\n",
    "#Convert Array of file name list into two array of coordinates (get1 & get2) + array of all exploded filenames\n",
    "def convertToCoordinatesArrays(fileNameArray):\n",
    "    crawledCoordinates1=[]\n",
    "    crawledCoordinates2=[]\n",
    "    explodedDataArray=[]\n",
    "    for i in range(len(fileNameArray)):\n",
    "        #Split name into array\n",
    "        splitToArray=explodeArray(fileNameArray[i], ']', '[')\n",
    "        explodedDataArray=explodedDataArray+[splitToArray]\n",
    "        if splitToArray[3] not in crawledCoordinates1:\n",
    "            crawledCoordinates1=crawledCoordinates1+[splitToArray[3]]\n",
    "        else:\n",
    "            crawledCoordinates2=crawledCoordinates2+[splitToArray[3]]\n",
    "    return crawledCoordinates1, crawledCoordinates2, explodedDataArray\n",
    "\n",
    "#Return JSON Crawled file statistics (including coordinates need to be retried)\n",
    "def crawledFileStatistics(folderPath, arrayOfFiles, explodedArrayOfFiles, arrayOfCoordinates, refFileSize):\n",
    "    #Loop for each files\n",
    "    md5CC1=[]\n",
    "    md5CC2=[]\n",
    "    fileSize1=[]\n",
    "    fileSize2=[]\n",
    "    \n",
    "    #Filling md5 and fileSize data from 2 samples for future analysis\n",
    "    for i in range(len(arrayOfCoordinates)):\n",
    "        sResultIdx1, sResultIdx2=searchFile(arrayOfCoordinates[i], explodedArrayOfFiles)\n",
    "        fileName1=folderPath+arrayOfFiles[sResultIdx1]\n",
    "        fileName2=folderPath+arrayOfFiles[sResultIdx2]\n",
    "        #Insert MD5\n",
    "        md5CC1=md5CC1+[md5(fileName1)]\n",
    "        md5CC2=md5CC2+[md5(fileName2)]\n",
    "        #Insert File Size\n",
    "        fileSize1=fileSize1+[fileSize(fileName1)]\n",
    "        fileSize2=fileSize2+[fileSize(fileName2)]\n",
    "        \n",
    "    #Check for files with different MD5\n",
    "    differentMD5=[]\n",
    "    for i in range(len(md5CC1)):\n",
    "        if md5CC1[i] != md5CC2[i]:\n",
    "            differentMD5=differentMD5+[i]\n",
    "    \n",
    "    #Check for file size anomaly\n",
    "    fileSizeAnomaly1=[]\n",
    "    fileSizeAnomaly2=[]\n",
    "    for i in range(len(fileSize1)):\n",
    "        if fileSize[i] >= refFileSize:\n",
    "            fileSizeAnomaly1=fileSizeAnomaly1+[i]\n",
    "        if fileSize2[i] >= refFileSize:\n",
    "            fileSizeAnomaly2=fileSizeAnomaly2+[i]\n",
    "    \n",
    "    #Performing slice analysis from MD5 and File Size Parameter\n",
    "    indexToRetry=differentMD5.copy()\n",
    "    for i in range(len(fileSizeAnomaly1)):\n",
    "        if fileSizeAnomaly1[i] not in indexToRetry:\n",
    "            indexToRetry=indexToRetry+[fileSizeAnomaly1[i]]\n",
    "    for i in range(len(fileSizeAnomaly2)):\n",
    "        if fileSizeAnomaly2[i] not in indexToRetry:\n",
    "            indexToRetry=indexToRetry+[fileSizeAnomaly2[i]]\n",
    "            \n",
    "    #Conversion from index-base-error-reporting to Coordinate to Retry\n",
    "    coordinatesToRetry=[]\n",
    "    for i in range(len(indexToRetry)):\n",
    "        coordinatesToRetry=coordinatesToRetry+[arrayOfCoordinates[indexToRetry[i]]]\n",
    "        \n",
    "    #Counting on land coordinates (based from file-size)\n",
    "    onLandCount=0\n",
    "    for i in range(len(fileSize1)):\n",
    "        if fileSize1[i] == 24185:\n",
    "            onLandCount=onLandCount+1\n",
    "        \n",
    "    #Returning JSON Format\n",
    "    result={\n",
    "        '_inputFolderPath':folderPath,\n",
    "        '_inputArrayOfFiles':arrayOfFiles,\n",
    "        '_inputExplodedArrayOfFiles':explodedArrayOfFiles,\n",
    "        '_inputArrayOfCoordinates':arrayOfCoordinates,\n",
    "        '_inputRefFileSize':refFileSize,\n",
    "        'dataLength':len(arrayOfFiles),\n",
    "        'coordinatesLength':len(arrayOfCoordinates),\n",
    "        'dataOnLand':onLandCount,\n",
    "        'percentageOnLand':onLandCount/len(fileSize1)*100,\n",
    "        'md5_1':md5CC1,\n",
    "        'md5_2':md5CC2,\n",
    "        'fileSize_1':fileSize1,\n",
    "        'fileSize_2':fileSize2,\n",
    "        'diffMD5_idx':differentMD5,\n",
    "        'fileSizeAnomaly_1':fileSizeAnomaly1,\n",
    "        'fileSizeAnomaly_2':fileSizeAnomaly2,\n",
    "        'indexToRetry':indexToRetry,\n",
    "        'coordinatesToRetry':coordinatesToRetry,\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "\n",
    "#Return True of False for specific files (using file Size and)\n",
    "def ifProper(folderPath, fileName):\n",
    "    #Merge address\n",
    "    filePath=folderPath+fileName\n",
    "    checkFileSize=fileSize(filePath)\n",
    "    \n",
    "    #Return true/false\n",
    "    if(checkFileSize >= 24185):\n",
    "        result=False\n",
    "    else:\n",
    "        result=True\n",
    "    return result    \n",
    "    \n",
    "#Return File Metadata from inputted FileName & Folder location\n",
    "def convertToMetadata(folderPath, fileName):\n",
    "    #Merge address\n",
    "    filePath=folderPath+fileName\n",
    "    \n",
    "    #Open file to edit extract coordinate(s)\n",
    "    eliminate=openFile(filePath)\n",
    "    \n",
    "    #Get File metadata | Latitude and Longitude (from crawled files), et cetera\n",
    "    latitude, longitude=exploitLatLong(fileName)\n",
    "    lat=coordShortener(latitude)\n",
    "    long=coordShortener(longitude)\n",
    "    \n",
    "    #Initial Condition (for replacing header(s))\n",
    "    header=' \\n     Lat       Lon        yyyy-mm-dd hh:mm:ss (UTC)     z(m)\\n \\n'\n",
    "    dSpace1='     '\n",
    "    dSpace2='   '\n",
    "    dSpace3='     '\n",
    "    dSpace4_pos='     '\n",
    "    dSpace4_neg='    '\n",
    "    #DateInit\n",
    "    startYear=2020\n",
    "    startMonth=10\n",
    "    startDay=1\n",
    "    numberOfDays=15\n",
    "    dateInit=hourDateGenerator(startYear,startMonth,startDay,numberOfDays)\n",
    "    \n",
    "    #Remove Header\n",
    "    eliminate=replaceWithBlank(eliminate, header)\n",
    "    \n",
    "    #Remove unused data from the crawled data\n",
    "    for i in range(len(dateInit)):\n",
    "        perLineReplaceParams=dSpace1+str(long)+dSpace2+str(lat)+dSpace3+str(dateInit[i])+dSpace4_pos\n",
    "        perLineReplaceParamsNegative=dSpace1+str(long)+dSpace2+str(lat)+dSpace3+str(dateInit[i])+dSpace4_neg\n",
    "        eliminate=replaceWithBlank(eliminate, perLineReplaceParams)\n",
    "        eliminate=replaceWithBlank(eliminate, perLineReplaceParamsNegative)\n",
    "        \n",
    "    #Convert Data to Array of Text\n",
    "    convertDataFromTextToArray=eliminate.split('\\n')\n",
    "    \n",
    "    #Delete last item to prevent float conversion error (blank data)\n",
    "    convertDataFromTextToArray=convertDataFromTextToArray[:-1]\n",
    "    \n",
    "    #Convert data from Text to Float\n",
    "    dataInFloat=[]\n",
    "    for i in range(len(convertDataFromTextToArray)):\n",
    "        try:\n",
    "            dataInFloat=dataInFloat+[float(convertDataFromTextToArray[i])]\n",
    "        except ValueError:\n",
    "            print('error', ' on line', i)\n",
    "    \n",
    "    #Constructing Metadata\n",
    "    metadata={\n",
    "        latitude+','+longitude{\n",
    "            'filename':fileName,\n",
    "            'md5':md5(filePath),\n",
    "            'lastModified':lastModified(filePath),\n",
    "            'fileSize':fileSize(filePath),\n",
    "            'latitude':latitude,\n",
    "            'longitude':longitude,\n",
    "            'lat_crawl':longitude,\n",
    "            'long_crawl':latitude,\n",
    "            'lat_crawl_short':long,\n",
    "            'long_crawl_short':lat,\n",
    "            'start_date':dateOffsetHours(startYear,startMonth,startDay,0),\n",
    "            'end_date':dateOffsetHours(startYear,startMonth,startDay,numberOfDays),\n",
    "            'date_range':numberOfDays,\n",
    "            'tidesData': dataInFloat\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "\n",
    "#Comparing and Append Data if Not Duplicated (Coordinates)\n",
    "def compareAppendCoor(dataArray, insert):\n",
    "    if insert not in dataArray:\n",
    "        dataArray=dataArray+[insert]\n",
    "    return dataArray\n",
    "\n",
    "#Make new Database\n",
    "\n",
    "#Write to Table\n",
    "def writeTb(dbPath, database, password, data):\n",
    "    #Constructing Path\n",
    "    dbRead=dbPath+database\n",
    "    import json\n",
    "    with open(dbRead, 'r') as readDb:\n",
    "        db=json.load(readDb)\n",
    "        \n",
    "    #write to table\n",
    "    tbPath=dbPath+db[password]['tablePath']+db[password]['table']\n",
    "    f=open(tbPath, 'w')\n",
    "    f.write(data)\n",
    "    f.close\n",
    "\n",
    "#Read Database\n",
    "def readTb(dbPath, database, password):\n",
    "    #Constructing Path\n",
    "    dbRead=dbPath+database\n",
    "    import json\n",
    "    with open(dbRead, 'r') as readDb:\n",
    "        db=json.load(readDb)\n",
    "        \n",
    "    #open table\n",
    "    tbPath=dbPath+db[password]['tablePath']+db[password]['table']\n",
    "    with open(tbRead, 'r') as readTb:\n",
    "        tb=json.load(readTb)\n",
    "    \n",
    "    return tb\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial Condition\n",
    "\n",
    "#Crawled data folder\n",
    "rawDataPath='../tides/test1/'\n",
    "\n",
    "#Processed Database\n",
    "dbPath='../tides/'\n",
    "password='OlrseTDW5Q0IINlQMtAWLqA9kugyWB'\n",
    "tablePath='YiJzexGqyHKyfYZpvL5b6vLp2Q8Od7/'\n",
    "database='RiqOYsHMvGfAhuL7NQjttQFR2dXV2R.json'\n",
    "table='yJ1MKQlIAeG5osozY1mhIJzGyL0eMI.json'\n",
    "\n",
    "#Coefficient\n",
    "referenceFileSize=25000 #bytes\n",
    "\n",
    "#Replaced header\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
